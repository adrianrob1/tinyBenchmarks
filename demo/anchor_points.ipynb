{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb0a714-9d15-41bc-ac06-ce5a1e448b1d",
   "metadata": {},
   "source": [
    "# Finding and using anchor points\n",
    "\n",
    "In this notebook, we show how to find anchor points based on your training set and how to use them to estimate the performance of new models in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008d96d-1ee5-44cf-91cb-293fb3e048bf",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d9b93-5059-416c-8a53-e3b4cc24a904",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7892164d-f5bb-4cef-9f4f-685a9af85679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from irt import *\n",
    "from utils import *\n",
    "\n",
    "random_state = 420"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb8ce2-1851-4131-8d35-36214be71085",
   "metadata": {},
   "source": [
    "The leaderboard dataset we will use is composed by six scenarios (sub-datasets):\n",
    "1. TruthfulQA\n",
    "1. GSM8K\n",
    "1. Winogrande\n",
    "1. ARC\n",
    "1. HellaSwag\n",
    "1. MMLU\n",
    "\n",
    "MMLU is further divided into sub-scenarios (e.g., abstract algebra, anatomy, etc). Let's check scenarios and sub-scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9810bb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'harness_truthfulqa_mc_0': ['harness_truthfulqa_mc_0'],\n",
       " 'gsm8k': ['harness_gsm8k_5'],\n",
       " 'winogrande': ['harness_winogrande_5'],\n",
       " 'arc': ['harness_arc_challenge_25'],\n",
       " 'hellaswag': ['harness_hellaswag_10'],\n",
       " 'mmlu': ['harness_hendrycksTest_abstract_algebra_5',\n",
       "  'harness_hendrycksTest_anatomy_5',\n",
       "  'harness_hendrycksTest_astronomy_5',\n",
       "  'harness_hendrycksTest_business_ethics_5',\n",
       "  'harness_hendrycksTest_clinical_knowledge_5',\n",
       "  'harness_hendrycksTest_college_biology_5',\n",
       "  'harness_hendrycksTest_college_chemistry_5',\n",
       "  'harness_hendrycksTest_college_computer_science_5',\n",
       "  'harness_hendrycksTest_college_mathematics_5',\n",
       "  'harness_hendrycksTest_college_medicine_5',\n",
       "  'harness_hendrycksTest_college_physics_5',\n",
       "  'harness_hendrycksTest_computer_security_5',\n",
       "  'harness_hendrycksTest_conceptual_physics_5',\n",
       "  'harness_hendrycksTest_econometrics_5',\n",
       "  'harness_hendrycksTest_electrical_engineering_5',\n",
       "  'harness_hendrycksTest_elementary_mathematics_5',\n",
       "  'harness_hendrycksTest_formal_logic_5',\n",
       "  'harness_hendrycksTest_global_facts_5',\n",
       "  'harness_hendrycksTest_high_school_biology_5',\n",
       "  'harness_hendrycksTest_high_school_chemistry_5',\n",
       "  'harness_hendrycksTest_high_school_computer_science_5',\n",
       "  'harness_hendrycksTest_high_school_european_history_5',\n",
       "  'harness_hendrycksTest_high_school_geography_5',\n",
       "  'harness_hendrycksTest_high_school_government_and_politics_5',\n",
       "  'harness_hendrycksTest_high_school_macroeconomics_5',\n",
       "  'harness_hendrycksTest_high_school_mathematics_5',\n",
       "  'harness_hendrycksTest_high_school_microeconomics_5',\n",
       "  'harness_hendrycksTest_high_school_physics_5',\n",
       "  'harness_hendrycksTest_high_school_psychology_5',\n",
       "  'harness_hendrycksTest_high_school_statistics_5',\n",
       "  'harness_hendrycksTest_high_school_us_history_5',\n",
       "  'harness_hendrycksTest_high_school_world_history_5',\n",
       "  'harness_hendrycksTest_human_aging_5',\n",
       "  'harness_hendrycksTest_human_sexuality_5',\n",
       "  'harness_hendrycksTest_international_law_5',\n",
       "  'harness_hendrycksTest_jurisprudence_5',\n",
       "  'harness_hendrycksTest_logical_fallacies_5',\n",
       "  'harness_hendrycksTest_machine_learning_5',\n",
       "  'harness_hendrycksTest_management_5',\n",
       "  'harness_hendrycksTest_marketing_5',\n",
       "  'harness_hendrycksTest_medical_genetics_5',\n",
       "  'harness_hendrycksTest_miscellaneous_5',\n",
       "  'harness_hendrycksTest_moral_disputes_5',\n",
       "  'harness_hendrycksTest_moral_scenarios_5',\n",
       "  'harness_hendrycksTest_nutrition_5',\n",
       "  'harness_hendrycksTest_philosophy_5',\n",
       "  'harness_hendrycksTest_prehistory_5',\n",
       "  'harness_hendrycksTest_professional_accounting_5',\n",
       "  'harness_hendrycksTest_professional_law_5',\n",
       "  'harness_hendrycksTest_professional_medicine_5',\n",
       "  'harness_hendrycksTest_professional_psychology_5',\n",
       "  'harness_hendrycksTest_public_relations_5',\n",
       "  'harness_hendrycksTest_security_studies_5',\n",
       "  'harness_hendrycksTest_sociology_5',\n",
       "  'harness_hendrycksTest_us_foreign_policy_5',\n",
       "  'harness_hendrycksTest_virology_5',\n",
       "  'harness_hendrycksTest_world_religions_5']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26499fc1-2bda-44b2-9131-e78d16f7f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_SCENARIOS = ['gsm8k', 'arc', 'hellaswag', 'harness_truthfulqa_mc_0']\n",
    "\n",
    "# select gsm8k, arc, hellaswag\n",
    "lb_scenarios = {'lb': []}\n",
    "for scenario in scenarios.keys():\n",
    "    if scenario in SELECTED_SCENARIOS:\n",
    "        lb_scenarios['lb'].append(scenarios[scenario][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61658085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lb': ['harness_truthfulqa_mc_0',\n",
       "  'harness_gsm8k_5',\n",
       "  'harness_arc_challenge_25',\n",
       "  'harness_hellaswag_10']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e5d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_scenario_to_readable = {}\n",
    "for scenario in scenarios.keys():\n",
    "    if scenario in SELECTED_SCENARIOS:\n",
    "        lb_scenario_to_readable[scenarios[scenario][0]] = scenario\n",
    "\n",
    "lb_scenario_to_readable['harness_truthfulqa_mc_0'] = 'truthfulqa'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e5620-bd45-4985-b390-a154843b4d6c",
   "metadata": {},
   "source": [
    "Loading leaderboard data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca68f5c-49de-4f75-92e5-de639059cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data/lb.pickle', 'rb') as handle:\n",
    "#    data = pickle.load(handle)\n",
    "with open('data/lb_scenarios.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5058b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harness_hellaswag_10\n",
      "Number of nan values in data: 0\n",
      "harness_truthfulqa_mc_0\n",
      "Number of nan values in data: 104\n",
      "harness_arc_challenge_25\n",
      "Number of nan values in data: 0\n",
      "harness_gsm8k_5\n",
      "Number of nan values in data: 0\n"
     ]
    }
   ],
   "source": [
    "for s in data['data'].keys():\n",
    "    print(s)\n",
    "    print('Number of nan values in data:', np.sum(np.isnan(data['data'][s]['correctness'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14f180-d322-4cd5-8d0c-4ae4fef04127",
   "metadata": {},
   "source": [
    "In this dataset, we have data from 395 models. Let's see the names of some of them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6c4201-0675-42e5-8a7a-8cf75592e661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " [['open-llm-leaderboard/details_moreh__MoMo-70B-lora-1.8.6-DPO',\n",
       "   'open-llm-leaderboard/details_cloudyu__Yi-34Bx3-MoE-90B',\n",
       "   'open-llm-leaderboard/details_Weyaxi__Helion-4x34B',\n",
       "   'open-llm-leaderboard/details_Weyaxi__Bagel-Hermes-34B-Slerp',\n",
       "   'open-llm-leaderboard/details_Weyaxi__Bagel-Hermes-2x34b',\n",
       "   'open-llm-leaderboard/details_nfaheem__Marcoroni-7b-DPO-Merge',\n",
       "   'open-llm-leaderboard/details_jondurbin__bagel-dpo-34b-v0.2',\n",
       "   'open-llm-leaderboard/details_udkai__Turdus',\n",
       "   'open-llm-leaderboard/details_gagan3012__MetaModel_moe',\n",
       "   'open-llm-leaderboard/details_jeonsworld__CarbonVillain-en-10.7B-v3',\n",
       "   'open-llm-leaderboard/details_TomGrc__FusionNet',\n",
       "   'open-llm-leaderboard/details_kekmodel__StopCarbon-10.7B-v6',\n",
       "   'open-llm-leaderboard/details_jeonsworld__CarbonVillain-en-10.7B-v1',\n",
       "   'open-llm-leaderboard/details_Weyaxi__SauerkrautLM-UNA-SOLAR-Instruct',\n",
       "   'open-llm-leaderboard/details_VAGOsolutions__SauerkrautLM-SOLAR-Instruct',\n",
       "   'open-llm-leaderboard/details_bhavinjawade__SOLAR-10B-Nector-DPO-Jawade',\n",
       "   'open-llm-leaderboard/details_kyujinpy__Sakura-SOLAR-Instruct-DPO-v2',\n",
       "   'open-llm-leaderboard/details_fblgit__UNA-SOLAR-10.7B-Instruct-v1.0',\n",
       "   'open-llm-leaderboard/details_kyujinpy__Sakura-SOLRCA-Instruct-DPO',\n",
       "   'open-llm-leaderboard/details_zhengr__MixTAO-7Bx2-MoE-DPO',\n",
       "   'open-llm-leaderboard/details_Weyaxi__Nous-Hermes-2-SUS-Chat-2x34B',\n",
       "   'open-llm-leaderboard/details_NousResearch__Nous-Hermes-2-Yi-34B',\n",
       "   'open-llm-leaderboard/details_flemmingmiguel__NeuDist-Ro-7B',\n",
       "   'open-llm-leaderboard/details_mlabonne__NeuralMarcoro14-7B',\n",
       "   'open-llm-leaderboard/details_cookinai__BruinHermes',\n",
       "   'open-llm-leaderboard/details_shadowml__Daredevil-7B',\n",
       "   'open-llm-leaderboard/details_zyh3826__GML-Mistral-merged-v1',\n",
       "   'open-llm-leaderboard/details_Sao10K__WinterGoddess-1.4x-70B-L2',\n",
       "   'open-llm-leaderboard/details_CultriX__MistralTrixTest',\n",
       "   'open-llm-leaderboard/details_rombodawg__Open_Gpt4_8x7B',\n",
       "   'open-llm-leaderboard/details_shadowml__Marcoro14-7B-ties',\n",
       "   'open-llm-leaderboard/details_VAGOsolutions__SauerkrautLM-Mixtral-8x7B-Instruct',\n",
       "   'open-llm-leaderboard/details_PSanni__MPOMixtral-8x7B-Instruct-v0.1',\n",
       "   'open-llm-leaderboard/details_maywell__PiVoT-SUS-RP',\n",
       "   'open-llm-leaderboard/details_rwitz2__pee',\n",
       "   'open-llm-leaderboard/details_Brillibits__Instruct_Mixtral-8x7B-v0.1_Dolly15K',\n",
       "   'open-llm-leaderboard/details_mindy-labs__mindy-7b',\n",
       "   'open-llm-leaderboard/details_janhq__supermario-slerp',\n",
       "   'open-llm-leaderboard/details_rishiraj__CatPPT-base',\n",
       "   'open-llm-leaderboard/details_SanjiWatsuki__Kunoichi-7B',\n",
       "   'open-llm-leaderboard/details_brucethemoose__Yi-34B-200K-DARE-merge-v5',\n",
       "   'open-llm-leaderboard/details_AA051611__A0110',\n",
       "   'open-llm-leaderboard/details_Weyaxi__openchat-3.5-1210-Seraph-Slerp',\n",
       "   'open-llm-leaderboard/details_SanjiWatsuki__Loyal-Macaroni-Maid-7B',\n",
       "   'open-llm-leaderboard/details_AA051610__A0106',\n",
       "   'open-llm-leaderboard/details_PulsarAI__OpenHermes-2.5-neural-chat-v3-3-Slerp',\n",
       "   'open-llm-leaderboard/details_Walmart-the-bag__Solar-10.7B-Cato',\n",
       "   'open-llm-leaderboard/details_Weyaxi__OpenHermes-2.5-neural-chat-v3-3-openchat-3.5-1210-Slerp',\n",
       "   'open-llm-leaderboard/details_Intel__neural-chat-7b-v3-3-Slerp',\n",
       "   'open-llm-leaderboard/details_KnutJaegersberg__Deacon-34b-Adapter',\n",
       "   'open-llm-leaderboard/details_TomGrc__FusionNet_SOLAR',\n",
       "   'open-llm-leaderboard/details_superlazycoder__NeuralPipe-7B-slerp',\n",
       "   'open-llm-leaderboard/details_NousResearch__Nous-Hermes-2-SOLAR-10.7B',\n",
       "   'open-llm-leaderboard/details_chanwit__flux-7b-v0.1',\n",
       "   'open-llm-leaderboard/details_one-man-army__una-neural-chat-v3-3-P2-OMA',\n",
       "   'open-llm-leaderboard/details_Q-bert__MetaMath-Cybertron',\n",
       "   'open-llm-leaderboard/details_Mihaiii__Pallas-0.2',\n",
       "   'open-llm-leaderboard/details_perlthoughts__Chupacabra-8x7B-MoE',\n",
       "   'open-llm-leaderboard/details_perlthoughts__Falkor-7b',\n",
       "   'open-llm-leaderboard/details_APMIC__caigun-lora-model-34B-v3',\n",
       "   'open-llm-leaderboard/details_Mihaiii__Pallas-0.5-LASER-0.1',\n",
       "   'open-llm-leaderboard/details_rishiraj__oswald-7b',\n",
       "   'open-llm-leaderboard/details_Mihaiii__Pallas-0.4',\n",
       "   'open-llm-leaderboard/details_flemmingmiguel__Distilled-HermesChat-7B',\n",
       "   'open-llm-leaderboard/details_Weyaxi__MetaMath-OpenHermes-2.5-neural-chat-v3-3-Slerp',\n",
       "   'open-llm-leaderboard/details_Intel__neural-chat-7b-v3-3',\n",
       "   'open-llm-leaderboard/details_migtissera__Tess-M-v1.3',\n",
       "   'open-llm-leaderboard/details_fblgit__una-cybertron-7b-v2-bf16',\n",
       "   'open-llm-leaderboard/details_chargoddard__mixtralmerge-8x7B-rebalanced-test',\n",
       "   'open-llm-leaderboard/details_FelixChao__WizardDolphin-7B',\n",
       "   'open-llm-leaderboard/details_FelixChao__ExtremeDolphin-MoE',\n",
       "   'open-llm-leaderboard/details_rishiraj__oswald-2x7b',\n",
       "   'open-llm-leaderboard/details_Sao10K__Sensualize-Mixtral-bf16',\n",
       "   'open-llm-leaderboard/details_OpenBuddy__openbuddy-deepseek-67b-v15-base',\n",
       "   'open-llm-leaderboard/details_diffnamehard__Mistral-CatMacaroni-slerp-gradient',\n",
       "   'open-llm-leaderboard/details_chargoddard__servile-harpsichord-cdpo',\n",
       "   'open-llm-leaderboard/details_AA051611__limb',\n",
       "   'open-llm-leaderboard/details_adamo1139__Yi-34B-AEZAKMI-v1',\n",
       "   'open-llm-leaderboard/details_jondurbin__spicyboros-70b-2.2',\n",
       "   'open-llm-leaderboard/details_mistralai__Mixtral-8x7B-v0.1',\n",
       "   'open-llm-leaderboard/details_kyujinpy__PlatYi-34B-Llama',\n",
       "   'open-llm-leaderboard/details_nlpguy__ColorShadow-7B',\n",
       "   'open-llm-leaderboard/details_Mihaiii__Pallas-0.5-LASER-0.4',\n",
       "   'open-llm-leaderboard/details_decapoda-research__Antares-11b-v1',\n",
       "   'open-llm-leaderboard/details_Sao10K__Sensualize-Solar-10.7B',\n",
       "   'open-llm-leaderboard/details_LoSboccacc__orthogonal-2x7B-base',\n",
       "   'open-llm-leaderboard/details_Azazelle__xDAN-SlimOrca',\n",
       "   'open-llm-leaderboard/details_Mihaiii__Pallas-0.5-LASER-exp2-0.1',\n",
       "   'open-llm-leaderboard/details_kyujinpy__PlatYi-34B-200k-Q-FastChat',\n",
       "   'open-llm-leaderboard/details_Swisslex__Mixtral-Orca-v0.1',\n",
       "   'open-llm-leaderboard/details_RatanRohith__MistralBeagle-RS-7B-V0.1',\n",
       "   'open-llm-leaderboard/details_mrfakename__NeuralOrca-7B-v1',\n",
       "   'open-llm-leaderboard/details_openaccess-ai-collective__DPOpenHermes-7B',\n",
       "   'open-llm-leaderboard/details_bongchoi__MoMo-70B-LoRA-V1.1',\n",
       "   'open-llm-leaderboard/details_Praneeth__StarMix-7B-slerp',\n",
       "   'open-llm-leaderboard/details_charlesdedampierre__TopicNeuralHermes-2.5-Mistral-7B',\n",
       "   'open-llm-leaderboard/details_diffnamehard__Mistral-CatMacaroni-slerp-uncensored',\n",
       "   'open-llm-leaderboard/details_beberik__rawr',\n",
       "   'open-llm-leaderboard/details_macadeliccc__laser-dolphin-mixtral-2x7b-dpo',\n",
       "   'open-llm-leaderboard/details_perlthoughts__Starling-LM-alpha-8x7B-MoE',\n",
       "   'open-llm-leaderboard/details_perlthoughts__Chupacabra-7B-v2',\n",
       "   'open-llm-leaderboard/details_Open-Orca__Mixtral-SlimOrca-8x7B',\n",
       "   'open-llm-leaderboard/details_macadeliccc__polyglot-math-4x7b',\n",
       "   'open-llm-leaderboard/details_Sao10K__Frostwind-10.7B-v1',\n",
       "   'open-llm-leaderboard/details_Mihaiii__Pallas-0.5-LASER-0.6',\n",
       "   'open-llm-leaderboard/details_Yhyu13__LMCocktail-Mistral-7B-v1',\n",
       "   'open-llm-leaderboard/details_rombodawg__Leaderboard-killer-MoE_4x7b',\n",
       "   'open-llm-leaderboard/details_augtoma__qCammel-70x',\n",
       "   'open-llm-leaderboard/details_Doctor-Shotgun__mythospice-limarp-70b',\n",
       "   'open-llm-leaderboard/details_chargoddard__mistral-11b-slimorca',\n",
       "   'open-llm-leaderboard/details_TokenBender__pic_7B_mistral_Full_v0.1',\n",
       "   'open-llm-leaderboard/details_TomGrc__FusionNet_passthrough',\n",
       "   'open-llm-leaderboard/details_perlthoughts__Chupacabra-7B-v2.03-128k',\n",
       "   'open-llm-leaderboard/details_TomGrc__FusionNet_passthrough_v0.1',\n",
       "   'open-llm-leaderboard/details_notbdq__alooowso',\n",
       "   'open-llm-leaderboard/details_Delcos__Velara-11B-V2',\n",
       "   'open-llm-leaderboard/details_jondurbin__bagel-7b-v0.1',\n",
       "   'open-llm-leaderboard/details_Mihaiii__Metis-0.3',\n",
       "   'open-llm-leaderboard/details_perlthoughts__Chupacabra-7B-v2.03',\n",
       "   'open-llm-leaderboard/details_SanjiWatsuki__neural-chat-7b-v3-3-wizardmath-dare-me',\n",
       "   'open-llm-leaderboard/details_simonveitner__MathHermes-2.5-Mistral-7B',\n",
       "   'open-llm-leaderboard/details_cognitivecomputations__dolphin-2.2.1-mistral-7b',\n",
       "   'open-llm-leaderboard/details_bn22__OpenHermes-2.5-Mistral-7B-MISALIGNED',\n",
       "   'open-llm-leaderboard/details_jarradh__llama2_70b_chat_uncensored',\n",
       "   'open-llm-leaderboard/details_Sao10K__Euryale-L2-70B',\n",
       "   'open-llm-leaderboard/details_jondurbin__airoboros-l2-70b-gpt4-m2.0',\n",
       "   'open-llm-leaderboard/details_upstage__llama-65b-instruct',\n",
       "   'open-llm-leaderboard/details_OpenAssistant__llama2-70b-oasst-sft-v10',\n",
       "   'open-llm-leaderboard/details_elinas__chronos-70b-v2',\n",
       "   'open-llm-leaderboard/details_Neuronovo__neuronovo-7B-v0.1',\n",
       "   'open-llm-leaderboard/details_openbmb__UltraLM-65b',\n",
       "   'open-llm-leaderboard/details_jae24__openhermes_dpo_norobot_0201',\n",
       "   'open-llm-leaderboard/details_KaeriJenti__Kaori-34B-v1',\n",
       "   'open-llm-leaderboard/details_argilla__notus-7b-v1',\n",
       "   'open-llm-leaderboard/details_xxyyy123__Mistral7B_adaptor_v1',\n",
       "   'open-llm-leaderboard/details_xDAN-AI__xDAN-L1Mix-DeepThinking-v2',\n",
       "   'open-llm-leaderboard/details_liuda1__dm7b_sft_gpt88w_merge',\n",
       "   'open-llm-leaderboard/details_KnutJaegersberg__Qwen-14B-Llamafied',\n",
       "   'open-llm-leaderboard/details_HenryJJ__dolphin-2.6-mistral-7b-dpo-orca-v3',\n",
       "   'open-llm-leaderboard/details_UCLA-AGI__test',\n",
       "   'open-llm-leaderboard/details_sr5434__CodegebraGPT-10b',\n",
       "   'open-llm-leaderboard/details_upaya07__Birbal-7B-V1',\n",
       "   'open-llm-leaderboard/details_migtissera__Tess-XS-v1-3-yarn-128K',\n",
       "   'open-llm-leaderboard/details_UCLA-AGI__test0',\n",
       "   'open-llm-leaderboard/details_Azazelle__Half-NSFW_Noromaid-7b',\n",
       "   'open-llm-leaderboard/details_migtissera__Tess-7B-v1.4',\n",
       "   'open-llm-leaderboard/details_kyujinpy__PlatYi-34B-200K-Q',\n",
       "   'open-llm-leaderboard/details_chargoddard__MelangeC-70b',\n",
       "   'open-llm-leaderboard/details_spmurrayzzz__Mistral-Syndicate-7B',\n",
       "   'open-llm-leaderboard/details_dfurman__Mistral-7B-Instruct-v0.2',\n",
       "   'open-llm-leaderboard/details_huangyt__Mistral-7B-v0.1-Open-Platypus_2.5w-r16-gate_up_down',\n",
       "   'open-llm-leaderboard/details_Intel__neural-chat-7b-v3-1',\n",
       "   'open-llm-leaderboard/details_TheBloke__robin-65b-v2-fp16',\n",
       "   'open-llm-leaderboard/details_microsoft__phi-2',\n",
       "   'open-llm-leaderboard/details_WizardLM__WizardLM-70B-V1.0',\n",
       "   'open-llm-leaderboard/details_huggingface__llama-65b',\n",
       "   'open-llm-leaderboard/details_kyujinpy__PlatYi-34B-Llama-Q-v3',\n",
       "   'open-llm-leaderboard/details_Dans-DiscountModels__Dans-07YahooAnswers-7b',\n",
       "   'open-llm-leaderboard/details_OpenBuddy__openbuddy-falcon-40b-v16.1-4k',\n",
       "   'open-llm-leaderboard/details_HiTZ__alpaca-lora-65b-en-pt-es-ca',\n",
       "   'open-llm-leaderboard/details_OpenBuddyEA__openbuddy-llama-30b-v7.1-bf16',\n",
       "   'open-llm-leaderboard/details_Sao10K__Zephyrus-L1-33B',\n",
       "   'open-llm-leaderboard/details_acrastt__kalomaze-stuff',\n",
       "   'open-llm-leaderboard/details_HenryJJ__Instruct_Mistral-7B-v0.1_Dolly15K',\n",
       "   'open-llm-leaderboard/details_speechlessai__speechless-mistral-7b-dare-0.85',\n",
       "   'open-llm-leaderboard/details_diffnamehard__Psyfighter2-Noromaid-ties-Capybara-13B',\n",
       "   'open-llm-leaderboard/details_CallComply__SOLAR-10.7B-Instruct-v1.0-128k',\n",
       "   'open-llm-leaderboard/details_teknium__CollectiveCognition-v1-Mistral-7B',\n",
       "   'open-llm-leaderboard/details_Mihaiii__Metis-0.1',\n",
       "   'open-llm-leaderboard/details_CallComply__Starling-LM-11B-alpha',\n",
       "   'open-llm-leaderboard/details_jilp00__Hermes-2-SOLAR-10.7B-Symbolic',\n",
       "   'open-llm-leaderboard/details_crumb__apricot-wildflower-20',\n",
       "   'open-llm-leaderboard/details_Locutusque__Orca-2-13B-no_robots',\n",
       "   'open-llm-leaderboard/details_maywell__Synatra-RP-Orca-2-7b-v0.1',\n",
       "   'open-llm-leaderboard/details_hywu__Camelidae-8x13B',\n",
       "   'open-llm-leaderboard/details_migtissera__SynthIA-7B-v1.3',\n",
       "   'open-llm-leaderboard/details_SuperAGI__SAM',\n",
       "   'open-llm-leaderboard/details_maywell__Synatra-7B-v0.3-RP',\n",
       "   'open-llm-leaderboard/details_bofenghuang__vigostral-7b-chat',\n",
       "   'open-llm-leaderboard/details_abdulrahman-nuzha__finetuned-Mistral-5000-v1.0',\n",
       "   'open-llm-leaderboard/details_lilloukas__Platypus-30B',\n",
       "   'open-llm-leaderboard/details_osanseviero__mistral-instruct-frankenmerge',\n",
       "   'open-llm-leaderboard/details_akjindal53244__Mistral-7B-v0.1-Open-Platypus',\n",
       "   'open-llm-leaderboard/details_jondurbin__airoboros-m-7b-3.1.2',\n",
       "   'open-llm-leaderboard/details_Aeala__GPT4-x-AlpacaDente2-30b',\n",
       "   'open-llm-leaderboard/details_PeanutJar__Mistral-v0.1-PeanutButter-v0.0.2-7B',\n",
       "   'open-llm-leaderboard/details_CobraMamba__mamba-gpt-7b-v1',\n",
       "   'open-llm-leaderboard/details_umd-zhou-lab__claude2-alpaca-13B',\n",
       "   'open-llm-leaderboard/details_Undi95__MLewd-ReMM-L2-Chat-20B',\n",
       "   'open-llm-leaderboard/details_Aspik101__trurl-2-13b-pl-instruct_unload',\n",
       "   'open-llm-leaderboard/details_ajibawa-2023__Uncensored-Frank-33B',\n",
       "   'open-llm-leaderboard/details_martyn__llama-megamerge-dare-13b',\n",
       "   'open-llm-leaderboard/details_Sao10K__Stheno-1.8-L2-13B',\n",
       "   'open-llm-leaderboard/details_Undi95__Mistral-11B-v0.1',\n",
       "   'open-llm-leaderboard/details_martyn__llama2-megamerge-dare-13b-v2',\n",
       "   'open-llm-leaderboard/details_oh-yeontaek__llama-2-13B-LoRA-assemble',\n",
       "   'open-llm-leaderboard/details_JosephusCheung__Pwen-14B-Chat-20_30',\n",
       "   'open-llm-leaderboard/details_Zangs3011__mistral_7b_DolphinCoder',\n",
       "   'open-llm-leaderboard/details_l3utterfly__mistral-7b-v0.1-layla-v1',\n",
       "   'open-llm-leaderboard/details_alignment-handbook__zephyr-7b-sft-full',\n",
       "   'open-llm-leaderboard/details_PocketDoc__Dans-AdventurousWinds-7b',\n",
       "   'open-llm-leaderboard/details_SkunkworksAI__Mistralic-7B-1',\n",
       "   'open-llm-leaderboard/details_Sao10K__BrainDerp2',\n",
       "   'open-llm-leaderboard/details_PulsarAI__2x-LoRA-Assemble-Nova-13B',\n",
       "   'open-llm-leaderboard/details_Undi95__MLewd-Chat-v2-13B',\n",
       "   'open-llm-leaderboard/details_jondurbin__airoboros-33b-gpt4-m2.0',\n",
       "   'open-llm-leaderboard/details_Undi95__ReMM-v2.2-L2-13B',\n",
       "   'open-llm-leaderboard/details_stabilityai__StableBeluga-13B',\n",
       "   'open-llm-leaderboard/details_WebraftAI__synapsellm-7b-mistral-v0.3-preview',\n",
       "   'open-llm-leaderboard/details_TheBloke__OpenOrca-Platypus2-13B-GPTQ',\n",
       "   'open-llm-leaderboard/details_huggingface__llama-30b',\n",
       "   'open-llm-leaderboard/details_Undi95__Emerald-13B',\n",
       "   'open-llm-leaderboard/details_TIGER-Lab__TIGERScore-13B',\n",
       "   'open-llm-leaderboard/details_Undi95__ReMM-v2.1-L2-13B',\n",
       "   'open-llm-leaderboard/details_chargoddard__storytime-13b',\n",
       "   'open-llm-leaderboard/details_BELLE-2__BELLE-Llama2-13B-chat-0.4M',\n",
       "   'open-llm-leaderboard/details_Brouz__Slerpeno',\n",
       "   'open-llm-leaderboard/details_PulsarAI__EnsembleV5-Nova-13B',\n",
       "   'open-llm-leaderboard/details_SciPhi__SciPhi-Self-RAG-Mistral-7B-32k',\n",
       "   'open-llm-leaderboard/details_Sao10K__Stheno-L2-13B',\n",
       "   'open-llm-leaderboard/details_uukuguy__speechless-code-mistral-7b-v2.0',\n",
       "   'open-llm-leaderboard/details_Gryphe__MythoMix-L2-13b',\n",
       "   'open-llm-leaderboard/details_Aspik101__StableBeluga-13B-instruct-PL-lora_unload',\n",
       "   'open-llm-leaderboard/details_Locutusque__Orca-2-13b-SFT-v6',\n",
       "   'open-llm-leaderboard/details_Austism__chronos-hermes-13b-v2',\n",
       "   'open-llm-leaderboard/details_The-Face-Of-Goonery__Huginn-13b-v1.2',\n",
       "   'open-llm-leaderboard/details_The-Face-Of-Goonery__huginnv1.2',\n",
       "   'open-llm-leaderboard/details_Undi95__Nous-Hermes-13B-Code',\n",
       "   'open-llm-leaderboard/details_YeungNLP__firefly-llama2-13b-v1.2',\n",
       "   'open-llm-leaderboard/details_Danielbrdz__Barcenas-13b',\n",
       "   'open-llm-leaderboard/details_lu-vae__llama2-13B-sharegpt4-orca-openplatypus-8w',\n",
       "   'open-llm-leaderboard/details_defog__sqlcoder-34b-alpha',\n",
       "   'open-llm-leaderboard/details_kingbri__chronolima-airo-grad-l2-13B',\n",
       "   'open-llm-leaderboard/details_Expert68__llama2_13b_instructed_version2',\n",
       "   'open-llm-leaderboard/details_mosaicml__mpt-30b-chat',\n",
       "   'open-llm-leaderboard/details_TFLai__Luban-Platypus2-13B-QLora-0.80-epoch',\n",
       "   'open-llm-leaderboard/details_ewqr2130__mistral-se-inst-ppo',\n",
       "   'open-llm-leaderboard/details_elinas__chronos-13b-v2',\n",
       "   'open-llm-leaderboard/details_Aspik101__vicuna-13b-v1.5-PL-lora_unload',\n",
       "   'open-llm-leaderboard/details_Sao10K__Mythical-Destroyer-V2-L2-13B',\n",
       "   'open-llm-leaderboard/details_jondurbin__airoboros-c34b-2.2.1',\n",
       "   'open-llm-leaderboard/details_PygmalionAI__pygmalion-2-13b',\n",
       "   'open-llm-leaderboard/details_ajibawa-2023__Python-Code-33B',\n",
       "   'open-llm-leaderboard/details_duliadotio__dulia-13b-8k-alpha',\n",
       "   'open-llm-leaderboard/details_lmsys__vicuna-13b-v1.5-16k',\n",
       "   'open-llm-leaderboard/details_WebraftAI__synapsellm-7b-mistral-v0.4-preview3',\n",
       "   'open-llm-leaderboard/details_digitous__13B-Chimera',\n",
       "   'open-llm-leaderboard/details_The-Face-Of-Goonery__Huginn-13b-FP16',\n",
       "   'open-llm-leaderboard/details_openaccess-ai-collective__manticore-13b',\n",
       "   'open-llm-leaderboard/details_ehartford__Samantha-1.11-CodeLlama-34b',\n",
       "   'open-llm-leaderboard/details_The-Face-Of-Goonery__Chronos-Beluga-v2-13bfp16',\n",
       "   'open-llm-leaderboard/details_elyza__ELYZA-japanese-Llama-2-13b-instruct',\n",
       "   'open-llm-leaderboard/details_Secbone__llama-2-13B-instructed',\n",
       "   'open-llm-leaderboard/details_TFLai__Nous-Hermes-Platypus2-13B-QLoRA-0.80-epoch',\n",
       "   'open-llm-leaderboard/details_BAAI__Aquila2-34B',\n",
       "   'open-llm-leaderboard/details_CallComply__zephyr-7b-beta-128k',\n",
       "   'open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-huangyt_Fintune_1_17w-q_k_v_o_proj',\n",
       "   'open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE3_3.3w-r16-gate_up_down',\n",
       "   'open-llm-leaderboard/details_totally-not-an-llm__EverythingLM-13b-V3-peft',\n",
       "   'open-llm-leaderboard/details_budecosystem__genz-13b-v2',\n",
       "   'open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE1_17w-r4',\n",
       "   'open-llm-leaderboard/details_TheBloke__Wizard-Vicuna-13B-Uncensored-HF',\n",
       "   'open-llm-leaderboard/details_hfl__chinese-alpaca-2-13b-16k',\n",
       "   'open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o',\n",
       "   'open-llm-leaderboard/details_TheBloke__airoboros-13B-HF',\n",
       "   'open-llm-leaderboard/details_jondurbin__airoboros-13b',\n",
       "   'open-llm-leaderboard/details_ehartford__based-30b',\n",
       "   'open-llm-leaderboard/details_Weyaxi__Platypus-Nebula-v2-7B',\n",
       "   'open-llm-leaderboard/details_euclaise__Ferret-7B',\n",
       "   'open-llm-leaderboard/details_zyh3826__llama2-13b-ft-openllm-leaderboard-v1',\n",
       "   'open-llm-leaderboard/details_Envoid__Libra-19B',\n",
       "   'open-llm-leaderboard/details_NobodyExistsOnTheInternet__GiftedConvo13bLoraNoEconsE4',\n",
       "   'open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE3_3.3w-r8-gate_up_down',\n",
       "   'open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-huangyt_FINETUNE2_3w',\n",
       "   'open-llm-leaderboard/details_shareAI__bimoGPT-llama2-13b',\n",
       "   'open-llm-leaderboard/details_chargoddard__llama2-22b-blocktriangular',\n",
       "   'open-llm-leaderboard/details_KnutJaegersberg__deacon-13b',\n",
       "   'open-llm-leaderboard/details_ehartford__WizardLM-1.0-Uncensored-CodeLlama-34b',\n",
       "   'open-llm-leaderboard/details_IGeniusDev__llama13B-quant8-testv1-openorca-customdataset',\n",
       "   'open-llm-leaderboard/details_psmathur__orca_mini_v3_7b',\n",
       "   'open-llm-leaderboard/details_TigerResearch__tigerbot-13b-base',\n",
       "   'open-llm-leaderboard/details_Aeala__GPT4-x-Alpasta-13b',\n",
       "   'open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE5_4w-r4-q_k_v_o',\n",
       "   'open-llm-leaderboard/details_kevinpro__Vicuna-13B-CoT',\n",
       "   'open-llm-leaderboard/details_AdaptLLM__finance-chat',\n",
       "   'open-llm-leaderboard/details_meta-math__MetaMath-Llemma-7B',\n",
       "   'open-llm-leaderboard/details_TFLai__Airboros2.1-Platypus2-13B-QLora-0.80-epoch',\n",
       "   'open-llm-leaderboard/details_Undi95__MLewd-L2-13B',\n",
       "   'open-llm-leaderboard/details_wei123602__llama2-13b-FINETUNE3_TEST',\n",
       "   'open-llm-leaderboard/details_KnutJaegersberg__Walter-Mistral-7B',\n",
       "   'open-llm-leaderboard/details_xzuyn__Alpacino-SuperCOT-13B',\n",
       "   'open-llm-leaderboard/details_dhmeltzer__Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged',\n",
       "   'open-llm-leaderboard/details_pe-nlp__llama-2-13b-platypus-vicuna-wizard',\n",
       "   'open-llm-leaderboard/details_HyperbeeAI__Tulpar-7b-v0',\n",
       "   'open-llm-leaderboard/details_teknium__Mistral-Trismegistus-7B',\n",
       "   'open-llm-leaderboard/details_heegyu__LIMA-13b-hf',\n",
       "   'open-llm-leaderboard/details_wahaha1987__llama_13b_sharegpt94k_fastchat',\n",
       "   'open-llm-leaderboard/details_wang7776__Llama-2-7b-chat-hf-10-sparsity',\n",
       "   'open-llm-leaderboard/details_camel-ai__CAMEL-13B-Combined-Data',\n",
       "   'open-llm-leaderboard/details_Unbabel__TowerInstruct-7B-v0.1',\n",
       "   'open-llm-leaderboard/details_beaugogh__Llama2-7b-openorca-mc-v2-dpo',\n",
       "   'open-llm-leaderboard/details_mncai__Llama2-7B-guanaco-1k',\n",
       "   'open-llm-leaderboard/details_HyperbeeAI__Tulpar-7b-v1',\n",
       "   'open-llm-leaderboard/details_OpenBuddy__openbuddy-mixtral-7bx8-v16.3-32k',\n",
       "   'open-llm-leaderboard/details_LTC-AI-Labs__L2-7b-Beluga-WVG-Test',\n",
       "   'open-llm-leaderboard/details_lmsys__vicuna-7b-v1.5',\n",
       "   'open-llm-leaderboard/details_ashercn97__manatee-7b',\n",
       "   'open-llm-leaderboard/details_umd-zhou-lab__recycled-wizardlm-7b-v2.0',\n",
       "   'open-llm-leaderboard/details_jphme__em_german_leo_mistral',\n",
       "   'open-llm-leaderboard/details_rombodawg__LosslessMegaCoder-llama2-7b-mini',\n",
       "   'open-llm-leaderboard/details_abhinand__tamil-llama-13b-instruct-v0.1',\n",
       "   'open-llm-leaderboard/details_jondurbin__airoboros-c34b-2.1',\n",
       "   'open-llm-leaderboard/details_camel-ai__CAMEL-13B-Role-Playing-Data',\n",
       "   'open-llm-leaderboard/details_Open-Orca__OpenOrca-Preview1-13B',\n",
       "   'open-llm-leaderboard/details_zarakiquemparte__kuchiki-l2-7b',\n",
       "   'open-llm-leaderboard/details_Locutusque__Rhino-Mistral-7B',\n",
       "   'open-llm-leaderboard/details_LTC-AI-Labs__L2-7b-Synthia-WVG-Test',\n",
       "   'open-llm-leaderboard/details_TheBloke__koala-13B-HF',\n",
       "   'open-llm-leaderboard/details_PygmalionAI__pygmalion-2-7b',\n",
       "   'open-llm-leaderboard/details_deepseek-ai__deepseek-moe-16b-base',\n",
       "   'open-llm-leaderboard/details_AlekseyKorshuk__vic15-exp-syn-fight-cp3838',\n",
       "   'open-llm-leaderboard/details_davzoku__cria-llama2-7b-v1.3',\n",
       "   'open-llm-leaderboard/details_YeungNLP__firefly-llama2-13b-pretrain',\n",
       "   'open-llm-leaderboard/details_wang7776__Mistral-7B-Instruct-v0.2-sparsity-20',\n",
       "   'open-llm-leaderboard/details_DopeorNope__LaOT',\n",
       "   'open-llm-leaderboard/details_maximuslee07__llama-2-7b-rockwell-final',\n",
       "   'open-llm-leaderboard/details_922-CA__monika-ddlc-7b-v1',\n",
       "   'open-llm-leaderboard/details_openthaigpt__openthaigpt-1.0.0-beta-13b-chat-hf',\n",
       "   'open-llm-leaderboard/details_NewstaR__Koss-7B-chat',\n",
       "   'open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-timedial',\n",
       "   'open-llm-leaderboard/details_TheBloke__tulu-7B-fp16',\n",
       "   'open-llm-leaderboard/details_kashif__stack-llama-2',\n",
       "   'open-llm-leaderboard/details_haoranxu__ALMA-13B',\n",
       "   'open-llm-leaderboard/details_togethercomputer__Llama-2-7B-32K-Instruct',\n",
       "   'open-llm-leaderboard/details_TinyPixel__testmodel2',\n",
       "   'open-llm-leaderboard/details_WizardLM__WizardMath-7B-V1.0',\n",
       "   'open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-mixed-datasets-time-unit',\n",
       "   'open-llm-leaderboard/details_bongchoi__test-llama2-7b',\n",
       "   'open-llm-leaderboard/details_TaylorAI__Flash-Llama-7B',\n",
       "   'open-llm-leaderboard/details_luffycodes__vicuna-class-shishya-ac-hal-13b-ep3',\n",
       "   'open-llm-leaderboard/details_HuggingFaceH4__starchat-beta',\n",
       "   'open-llm-leaderboard/details_clibrain__Llama-2-7b-ft-instruct-es',\n",
       "   'open-llm-leaderboard/details_dotvignesh__perry-7b',\n",
       "   'open-llm-leaderboard/details_heegyu__LIMA2-7b-hf',\n",
       "   'open-llm-leaderboard/details_PocketDoc__Dans-RetroRodeo-13b',\n",
       "   'open-llm-leaderboard/details_martyn__mistral-megamerge-dare-7b',\n",
       "   'open-llm-leaderboard/details_dhmeltzer__llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged',\n",
       "   'open-llm-leaderboard/details_GOAT-AI__GOAT-7B-Community',\n",
       "   'open-llm-leaderboard/details_jondurbin__airoboros-7b-gpt4-1.1',\n",
       "   'open-llm-leaderboard/details_llm-agents__tora-7b-v1.0',\n",
       "   'open-llm-leaderboard/details_cognitivecomputations__yayi2-30b-llama',\n",
       "   'open-llm-leaderboard/details_TheBloke__Wizard-Vicuna-7B-Uncensored-HF',\n",
       "   'open-llm-leaderboard/details_jondurbin__airoboros-l2-7b-gpt4-m2.0',\n",
       "   'open-llm-leaderboard/details_DevaMalla__llama_7b_qlora_pds-eval',\n",
       "   'open-llm-leaderboard/details_webbigdata__ALMA-7B-Ja-V2',\n",
       "   'open-llm-leaderboard/details_bofenghuang__vigogne-7b-instruct',\n",
       "   'open-llm-leaderboard/details_Neko-Institute-of-Science__metharme-7b',\n",
       "   'open-llm-leaderboard/details_jondurbin__airoboros-7b',\n",
       "   'open-llm-leaderboard/details_h2m__mhm-7b-v1.3',\n",
       "   'open-llm-leaderboard/details_Undi95__Mixtral-8x7B-MoE-RP-Story',\n",
       "   'open-llm-leaderboard/details_itsliupeng__openllama-7b-base',\n",
       "   'open-llm-leaderboard/details_ausboss__llama7b-wizardlm-unfiltered',\n",
       "   'open-llm-leaderboard/details_DevaMalla__llama_7b_lora',\n",
       "   'open-llm-leaderboard/details_ehartford__dolphin-2.2-yi-34b-200k',\n",
       "   'open-llm-leaderboard/details_cognitivecomputations__dolphin-2.2-yi-34b-200k',\n",
       "   'open-llm-leaderboard/details_jondurbin__airoboros-7b-gpt4-1.4.1-qlora',\n",
       "   'open-llm-leaderboard/details_YeungNLP__firefly-llama2-7b-pretrain',\n",
       "   'open-llm-leaderboard/details_fireballoon__baichuan-vicuna-chinese-7b',\n",
       "   'open-llm-leaderboard/details_vikash06__mistral_v1',\n",
       "   'open-llm-leaderboard/details_huggingface__llama-7b',\n",
       "   'open-llm-leaderboard/details_yeontaek__WizardCoder-Python-13B-LoRa',\n",
       "   'open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-mctaco-modified1',\n",
       "   'open-llm-leaderboard/details_ashercn97__giraffe-7b',\n",
       "   'open-llm-leaderboard/details_luffycodes__llama-shishya-7b-ep3-v1',\n",
       "   'open-llm-leaderboard/details_shareAI__CodeLLaMA-chat-13b-Chinese',\n",
       "   'open-llm-leaderboard/details_KnutJaegersberg__Qwen-1_8B-Llamafied',\n",
       "   'open-llm-leaderboard/details_WeOpenML__Alpaca-7B-v1',\n",
       "   'open-llm-leaderboard/details_mosaicml__mpt-7b',\n",
       "   'open-llm-leaderboard/details_togethercomputer__GPT-JT-6B-v0',\n",
       "   'open-llm-leaderboard/details_hyunseoki__ko-ref-llama2-13b',\n",
       "   'open-llm-leaderboard/details_cyberagent__calm2-7b-chat',\n",
       "   'open-llm-leaderboard/details_FreedomIntelligence__phoenix-inst-chat-7b',\n",
       "   'open-llm-leaderboard/details_Pierre-obi__Mistral_solar-slerp',\n",
       "   'open-llm-leaderboard/details_qblocks__codellama_7b_DolphinCoder',\n",
       "   'open-llm-leaderboard/details_openlm-research__open_llama_7b',\n",
       "   'open-llm-leaderboard/details_klosax__open_llama_13b_600bt_preview',\n",
       "   'open-llm-leaderboard/details_wenge-research__yayi-7b',\n",
       "   'open-llm-leaderboard/details_glaiveai__glaive-coder-7b',\n",
       "   'open-llm-leaderboard/details_uukuguy__speechless-coder-ds-6.7b',\n",
       "   'open-llm-leaderboard/details_digitous__Javalion-R',\n",
       "   'open-llm-leaderboard/details_NousResearch__CodeLlama-34b-hf',\n",
       "   'open-llm-leaderboard/details_codellama__CodeLlama-7b-hf',\n",
       "   'open-llm-leaderboard/details_heegyu__RedTulu-Uncensored-3B-0719']])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['models']),data['models'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8135b-e9ec-468a-85a7-3cd3fc3d31fe",
   "metadata": {},
   "source": [
    "Below, we will process the data so all correctness scores (for all scenarios) are stored in $Y$. The dictionaries `scenarios_position` and `subscenarios_position` give the position of scenarios/subscenarios correctness scores in $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee09c25b-2dc4-4403-a972-9fb05cfe917b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393, 13350)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_position, subscenarios_position = prepare_data(lb_scenarios, data)\n",
    "Y = create_responses(lb_scenarios, data)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a96d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 13349)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_position['lb'][0], scenarios_position['lb'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6bd2d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['harness_truthfulqa_mc_0', 'harness_gsm8k_5', 'harness_arc_challenge_25', 'harness_hellaswag_10'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscenarios_position['lb'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002485a-1e82-409b-aaf2-ddb6a82bc315",
   "metadata": {},
   "source": [
    "For example, below you can see the scores for MMLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4dd9649-ba75-49c0-92fe-b00d2afc252e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.99999999, 0.99994655, 1.        , ..., 1.        , 0.        ,\n",
       "         1.        ],\n",
       "        [0.99984026, 0.99999999, 0.99908489, ..., 1.        , 1.        ,\n",
       "         1.        ],\n",
       "        [0.99937792, 0.99999997, 0.99732544, ..., 1.        , 1.        ,\n",
       "         1.        ],\n",
       "        ...,\n",
       "        [0.59677787, 0.99801392, 0.20934594, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.67288482, 0.99814252, 0.3865014 , ..., 0.        , 1.        ,\n",
       "         1.        ],\n",
       "        [0.42345796, 0.99926741, 0.82057698, ..., 0.        , 0.        ,\n",
       "         0.        ]]),\n",
       " (393, 13350))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:,scenarios_position['lb']], Y[:,scenarios_position['lb']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49449232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count nan values in Y\n",
    "np.sum(np.isnan(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa753f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y stats:\n",
      "min: 0.0\n",
      "max: 1.0000000000000002\n",
      "mean: 0.7231160348021274\n",
      "std: 0.443520043003996\n"
     ]
    }
   ],
   "source": [
    "# fill nan values with 0\n",
    "Y[np.isnan(Y)] = 0\n",
    "\n",
    "# print stats of Y\n",
    "print('Y stats:')\n",
    "print('min:', np.min(Y))\n",
    "print('max:', np.max(Y))\n",
    "print('mean:', np.mean(Y))\n",
    "print('std:', np.std(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23004a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['harness_truthfulqa_mc_0', 'harness_gsm8k_5', 'harness_arc_challenge_25', 'harness_hellaswag_10'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscenarios_position['lb'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c54b91d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lb harness_truthfulqa_mc_0 0 816\n",
      "lb harness_gsm8k_5 817 2135\n",
      "lb harness_arc_challenge_25 2136 3307\n",
      "lb harness_hellaswag_10 3308 13349\n"
     ]
    }
   ],
   "source": [
    "for s in subscenarios_position.keys():\n",
    "    for k in subscenarios_position[s].keys():\n",
    "        print(s, k, subscenarios_position[s][k][0], subscenarios_position[s][k][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662681a7-10b0-4ddc-a692-52d278499539",
   "metadata": {},
   "source": [
    "For scenarios that have multiple subscenarios, it is usually the case that we want to give equal importance to individual subscenarios when computing the aggregated performance in that scenario. This is equivalent to using a weighted average when computing the aggregated performance. We will create `balance_weights`, a vector of weights to help us compute those weighted averages. These weights will be different than one only for MMLU, which is the only scenario with multiple subscenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f40fc53-b11e-41cc-adc2-7abff1a2b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_weights = np.ones(Y.shape[1])\n",
    "\n",
    "N = len(scenarios_position['lb'])\n",
    "n_sub = len(lb_scenarios['lb'])\n",
    "for sub in lb_scenarios['lb']:\n",
    "    n_i = len(subscenarios_position['lb'][sub])\n",
    "    balance_weights[subscenarios_position['lb'][sub]] = N/(n_sub*n_i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d42b7-c6ac-4695-a5f0-3087c091d16d",
   "metadata": {},
   "source": [
    "We can see below that first averaging within subscenarios and then computing a simple average is equivalent to using a weighted average from the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7b51b6f-5ce5-46bf-ba44-836386db05f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1356762293373007e-14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs1 = np.mean([Y[:,subscenarios_position['lb'][sub]].mean(axis=1) for sub in lb_scenarios['lb']], axis=0)\n",
    "accs2 = (balance_weights*Y)[:,scenarios_position['lb']].mean(axis=1)\n",
    "\n",
    "np.abs(accs1 - accs2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b02ccf",
   "metadata": {},
   "source": [
    "## Create IRT Model for whole response dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "719e3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_IRT_dimension(Y_bin_train, Y_train, scenarios, Ds, device, epochs, lr, scenarios_position, subscenarios_position):\n",
    "    val_ind = list(range(0,Y_bin_train.shape[0],5)) # Validation indices\n",
    "    train_ind = [i for i in range(Y_bin_train.shape[0]) if i not in val_ind]\n",
    "\n",
    "    # Saving the training dataset in the needed format\n",
    "    create_irt_dataset(Y_bin_train[train_ind], 'data/irt_val_dataset.jsonlines')\n",
    "\n",
    "    # Trying different Ds\n",
    "    errors = []  \n",
    "    errors2 = []\n",
    "\n",
    "    for D in tqdm(Ds):\n",
    "        dataset_name = 'data/irt_val_dataset.jsonlines'\n",
    "        model_name = 'data/irt_val_model/'\n",
    "        \n",
    "        # Load trained IRT model parameters\n",
    "        train_irt_model(dataset_name, model_name, D, lr, epochs, device)\n",
    "        A, B, Theta = load_irt_parameters(model_name)\n",
    "        \n",
    "        # Determine seen and unseen items for validation\n",
    "        seen_items = list(range(0, Y_bin_train.shape[1], 2))\n",
    "        unseen_items = list(range(1, Y_bin_train.shape[1], 2))\n",
    "\n",
    "        # Estimate ability parameters for the validation set\n",
    "        thetas = [estimate_ability_parameters(Y_bin_train[val_ind][j][seen_items], A[:, :, seen_items], B[:, :, seen_items]) for j in range(len(val_ind))]\n",
    "\n",
    "        # Compute validation errors for each scenario and update the errors list (in the end, we give the same weight for all scenarios)\n",
    "        errors2.append([])\n",
    "        for scenario in scenarios.keys():\n",
    "            balance_weights = np.ones(Y.shape[1])\n",
    "\n",
    "            selected_subscenarios = scenarios[scenario]\n",
    "\n",
    "            N = len(scenarios_position)\n",
    "            n_sub = len(selected_subscenarios)\n",
    "            for sub in selected_subscenarios:\n",
    "                n_i = len(subscenarios_position[scenario][sub])\n",
    "                balance_weights[subscenarios_position[scenario][sub]] = N/(n_sub*n_i) \n",
    "            \n",
    "            ind = [u for u in unseen_items if u in scenarios_position[scenario]]\n",
    "            errors2[-1].append(np.mean([abs((balance_weights*item_curve(thetas[j], A, B))[0,ind].mean()-Y_train[val_ind][j,ind].mean())for j in range(len(val_ind))]))\n",
    "        errors.append(np.mean(errors2[-1]))\n",
    "\n",
    "    return errors, errors2, Ds\n",
    "\n",
    "def train_IRT_for_scenarios(Y_bin_train, Y_train, all_scenarios, scenarios_position, subscenarios_position):\n",
    "    Ds = [15,] #2,5,10,] # Dimensions to try TODO: uncomment the rest of the dimensions\n",
    "    device = 'cuda' # Either 'cuda' or 'cpu' \n",
    "    epochs = 2000  # Number of epochs for IRT model training (py-irt default is 2000)\n",
    "    lr = .1  # Learning rate for IRT model training (py-irt default is .1)\n",
    "\n",
    "    errors, errors2, Ds = validate_IRT_dimension(Y_bin_train, Y_train, all_scenarios, Ds, device, epochs, lr, scenarios_position, subscenarios_position)\n",
    "\n",
    "    ind_D = np.argmin(np.array(errors))\n",
    "    D = Ds[ind_D]\n",
    "\n",
    "    create_irt_dataset(Y_bin_train, 'data/irt_dataset.jsonlines')\n",
    "\n",
    "    train_irt_model(dataset_name='data/irt_dataset.jsonlines', \n",
    "                model_name=f'data/irt_model_lb/', \n",
    "                D=D, lr=lr, epochs=epochs, device=device)\n",
    "    \n",
    "    return errors, errors2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32682af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:01<00:00, 63.01it/s]\n",
      "100%|| 1/1 [00:01<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y[:100]\n",
    "Y_train = Y[100:]\n",
    "\n",
    "Y_bin_train = np.zeros(Y_train.shape)\n",
    "Y_bin_test = np.zeros(Y_test.shape)\n",
    "\n",
    "cs = np.linspace(0.01,.99,100)  # Threshold values to consider\n",
    "for scenario in tqdm(lb_scenarios.keys()):\n",
    "    ind = scenarios_position[scenario]\n",
    "    # Find the best threshold value that minimizes the difference between averages\n",
    "    c = cs[np.argmin([np.mean((np.abs((Y_train[:,ind]>c).mean(axis=1)-Y_train[:,ind].mean(axis=1)))) for c in tqdm(cs)])]\n",
    "    # Apply the threshold to train and test responses\n",
    "    Y_bin_train[:,ind] = (Y_train[:,ind]>c).astype(int)\n",
    "    Y_bin_test[:,ind] = (Y_test[:,ind]>c).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b54181f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lb': ['harness_truthfulqa_mc_0',\n",
       "  'harness_gsm8k_5',\n",
       "  'harness_arc_challenge_25',\n",
       "  'harness_hellaswag_10']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05158845",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scenarios = {\n",
    "    'truthfulqa': ['harness_truthfulqa_mc_0'],\n",
    "    'hellaswag': ['harness_hellaswag_10'],\n",
    "    'arc': ['harness_arc_challenge_25'],\n",
    "    'gsm8k': ['harness_gsm8k_5']\n",
    "}\n",
    "\n",
    "scenarios_pos = {}\n",
    "subs_position = {}\n",
    "for s in lb_scenarios['lb']:\n",
    "    scenarios_pos[lb_scenario_to_readable[s]] = subscenarios_position['lb'][s]\n",
    "    subs_position[lb_scenario_to_readable[s]] = {s: subscenarios_position['lb'][s]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50266422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['truthfulqa', 'gsm8k', 'arc', 'hellaswag'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_position.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80733d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:32:55] config: model_type='multidim_2pl' epochs=2000              cli.py:109\n",
      "           priors='hierarchical' initializers=[] dims=15 lr=0.1                 \n",
      "           lr_decay=0.9999 dropout=0.5 hidden=100 vocab_size=None               \n",
      "           log_every=200 seed=42 deterministic=True                             \n",
      "           data_path: data/irt_val_dataset.jsonlines                  cli.py:111\n",
      "           output directory: data/irt_val_model/                      cli.py:112\n",
      "[18:32:56] amortized: False                                       dataset.py:112\n",
      "[18:33:03] Vocab size: None                                       training.py:90\n",
      "[18:33:04] Training Model...                                          cli.py:116\n",
      "[18:33:04] args: {'device': 'cuda', 'num_items': 13350,          training.py:134\n",
      "           'num_subjects': 234}                                                 \n",
      "           Parsed Model Args: {'device': 'cuda', 'num_items':    training.py:147\n",
      "           13350, 'num_subjects': 234, 'priors': 'hierarchical',                \n",
      "           'dims': 15, 'dropout': 0.5, 'hidden': 100,                           \n",
      "           'vocab_size': None}                                                  \n",
      "torch.Size([3123900]) torch.Size([3123900])\n",
      "Training Pyro IRT Model for 2000 epochs\n",
      "\n",
      " Epoch  Loss           Best Loss      New LR \n",
      "\n",
      " 1      19670102.0512  19670102.0512  0.1000 \n",
      " 201    718240.7543    677873.1127    0.0980 \n",
      " 401    600490.5372    597114.2692    0.0961 \n",
      " 601    588115.8223    579192.8471    0.0942 \n",
      " 801    570149.5867    567210.5605    0.0923 \n",
      " 1001   568871.6052    563628.9631    0.0905 \n",
      " 1201   563530.1593    560960.6720    0.0887 \n",
      " 1401   560317.5620    559912.5130    0.0869 \n",
      " 1601   561277.4212    559416.8588    0.0852 \n",
      " 1801   561226.8090    558001.2484    0.0835 \n",
      " 2000   562049.0043    558001.2484    0.0819 \n",
      "[18:34:01] Train time: 66.06508660316467                              cli.py:122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [01:32<00:00, 92.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:31] config: model_type='multidim_2pl' epochs=2000              cli.py:109\n",
      "           priors='hierarchical' initializers=[] dims=15 lr=0.1                 \n",
      "           lr_decay=0.9999 dropout=0.5 hidden=100 vocab_size=None               \n",
      "           log_every=200 seed=42 deterministic=True                             \n",
      "           data_path: data/irt_dataset.jsonlines                      cli.py:111\n",
      "           output directory: data/irt_model_lb/                       cli.py:112\n",
      "[18:34:32] amortized: False                                       dataset.py:112\n",
      "[18:34:41] Vocab size: None                                       training.py:90\n",
      "[18:34:42] Training Model...                                          cli.py:116\n",
      "[18:34:42] args: {'device': 'cuda', 'num_items': 13350,          training.py:134\n",
      "           'num_subjects': 293}                                                 \n",
      "           Parsed Model Args: {'device': 'cuda', 'num_items':    training.py:147\n",
      "           13350, 'num_subjects': 293, 'priors': 'hierarchical',                \n",
      "           'dims': 15, 'dropout': 0.5, 'hidden': 100,                           \n",
      "           'vocab_size': None}                                                  \n",
      "torch.Size([3911550]) torch.Size([3911550])\n",
      "Training Pyro IRT Model for 2000 epochs\n",
      "\n",
      " Epoch  Loss           Best Loss      New LR \n",
      "\n",
      " 1      24588428.8930  24588428.8930  0.1000 \n",
      " 201    890869.5833    853267.8176    0.0980 \n",
      " 401    752266.8205    748096.1915    0.0961 \n",
      " 601    731999.1778    722872.7958    0.0942 \n",
      " 801    710759.0294    708347.0608    0.0923 \n",
      " 1001   708127.9465    703035.0615    0.0905 \n",
      " 1201   701306.9524    699496.3613    0.0887 \n",
      " 1401   697920.4481    697749.1727    0.0869 \n",
      " 1601   698861.9451    696573.1145    0.0852 \n",
      " 1801   698127.7400    695511.1748    0.0835 \n",
      " 2000   698566.4625    694229.9377    0.0819 \n",
      "[18:35:46] Train time: 75.08583879470825                              cli.py:122\n"
     ]
    }
   ],
   "source": [
    "err, err2 = train_IRT_for_scenarios(Y_bin_train, Y_train, all_scenarios, scenarios_pos, subs_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ead477d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5087422991787031],\n",
       " [[0.4745979047501622,\n",
       "   0.7885986845537435,\n",
       "   0.5788823331096964,\n",
       "   0.19289027430121009]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err, err2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106b620-7fe0-49bb-a8ac-3a946c15f751",
   "metadata": {},
   "source": [
    "## Getting and using anchor points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c4412-ae69-4184-b106-191a1c151736",
   "metadata": {},
   "source": [
    "Let's split the data in train and test (recent models are placed in the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc9874c5-7cb5-425b-8c41-9a87d7615ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y[:100]\n",
    "Y_train = Y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f485b83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6547273254796847"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(balance_weights*Y_train)[:,scenarios_position['lb']].mean(axis=1).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680d6e6-1ec2-4a24-a898-f29bd5ec109e",
   "metadata": {},
   "source": [
    "The variable `number_item` gives the number of anchor points we want to find in each scenario:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528f89a-64bb-497e-b993-9181996d75d1",
   "metadata": {},
   "source": [
    "The variable `clustering` specified how the clusting is run. If `clustering=\"correct.\"`, then correctness is used. On the other hand, if `clustering=\"irt\"`, then the IRT embeddings for examples are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5edf1d-21b3-46f9-9034-479ebe89314d",
   "metadata": {},
   "source": [
    "Computing anchor points and their weights for each scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "313c85b8-838d-416c-ac7d-e40725e08853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anchor_points(clustering, selected_scenarios, scenarios_position, balance_weights, Y_train, number_item, random_state):\n",
    "    anchor_points = {}\n",
    "    anchor_weights = {}\n",
    "\n",
    "    for scenario in selected_scenarios:\n",
    "\n",
    "        if clustering=='correct.':\n",
    "            X = Y_train[:,scenarios_position[scenario]].T\n",
    "        elif clustering=='irt':\n",
    "            A, B, _ = load_irt_parameters(f'data/irt_model_lb/')\n",
    "            X = np.vstack((A.squeeze(), B.squeeze().reshape((1,-1)))).T\n",
    "            X = X[scenarios_position[scenario]]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        #Normalizing balance_weights, so their sum is one within each scenario\n",
    "        norm_balance_weights = balance_weights[scenarios_position[scenario]]\n",
    "        norm_balance_weights /= norm_balance_weights.sum()\n",
    "\n",
    "        # Fitting the KMeans model\n",
    "        kmeans = KMeans(n_clusters=number_item, n_init=\"auto\", random_state=random_state)\n",
    "        kmeans.fit(X, sample_weight=norm_balance_weights)\n",
    "\n",
    "        # Calculating anchor points\n",
    "        anchor_points[scenario] = pairwise_distances(kmeans.cluster_centers_, X, metric='euclidean').argmin(axis=1)\n",
    "\n",
    "        # Calculating anchor weights\n",
    "        anchor_weights[scenario] = np.array([np.sum(norm_balance_weights[kmeans.labels_==c]) for c in range(number_item)])\n",
    "\n",
    "    return anchor_points, anchor_weights\n",
    "\n",
    "def estimate_lambdas(err, Y_train, scenarios_position, number_item):\n",
    "    lambds = {} \n",
    "\n",
    "    for i,scenario in tqdm(enumerate(scenarios_position.keys())):\n",
    "        v = np.var(Y_train[:,scenarios_position[scenario]], axis=1).mean()\n",
    "        b = np.mean(err[i]) \n",
    "        lambds[scenario] = get_lambda(b, v/(4*number_item))\n",
    "\n",
    "    return lambds\n",
    "\n",
    "def get_lambda(b, v):\n",
    "    return (b**2)/(v+(b**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d59f027c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13349"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscenarios_position['lb']['harness_hellaswag_10'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20a7ed46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lb': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  477,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  591,\n",
       "  592,\n",
       "  593,\n",
       "  594,\n",
       "  595,\n",
       "  596,\n",
       "  597,\n",
       "  598,\n",
       "  599,\n",
       "  600,\n",
       "  601,\n",
       "  602,\n",
       "  603,\n",
       "  604,\n",
       "  605,\n",
       "  606,\n",
       "  607,\n",
       "  608,\n",
       "  609,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  613,\n",
       "  614,\n",
       "  615,\n",
       "  616,\n",
       "  617,\n",
       "  618,\n",
       "  619,\n",
       "  620,\n",
       "  621,\n",
       "  622,\n",
       "  623,\n",
       "  624,\n",
       "  625,\n",
       "  626,\n",
       "  627,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  639,\n",
       "  640,\n",
       "  641,\n",
       "  642,\n",
       "  643,\n",
       "  644,\n",
       "  645,\n",
       "  646,\n",
       "  647,\n",
       "  648,\n",
       "  649,\n",
       "  650,\n",
       "  651,\n",
       "  652,\n",
       "  653,\n",
       "  654,\n",
       "  655,\n",
       "  656,\n",
       "  657,\n",
       "  658,\n",
       "  659,\n",
       "  660,\n",
       "  661,\n",
       "  662,\n",
       "  663,\n",
       "  664,\n",
       "  665,\n",
       "  666,\n",
       "  667,\n",
       "  668,\n",
       "  669,\n",
       "  670,\n",
       "  671,\n",
       "  672,\n",
       "  673,\n",
       "  674,\n",
       "  675,\n",
       "  676,\n",
       "  677,\n",
       "  678,\n",
       "  679,\n",
       "  680,\n",
       "  681,\n",
       "  682,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  689,\n",
       "  690,\n",
       "  691,\n",
       "  692,\n",
       "  693,\n",
       "  694,\n",
       "  695,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  699,\n",
       "  700,\n",
       "  701,\n",
       "  702,\n",
       "  703,\n",
       "  704,\n",
       "  705,\n",
       "  706,\n",
       "  707,\n",
       "  708,\n",
       "  709,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  713,\n",
       "  714,\n",
       "  715,\n",
       "  716,\n",
       "  717,\n",
       "  718,\n",
       "  719,\n",
       "  720,\n",
       "  721,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  726,\n",
       "  727,\n",
       "  728,\n",
       "  729,\n",
       "  730,\n",
       "  731,\n",
       "  732,\n",
       "  733,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  737,\n",
       "  738,\n",
       "  739,\n",
       "  740,\n",
       "  741,\n",
       "  742,\n",
       "  743,\n",
       "  744,\n",
       "  745,\n",
       "  746,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  750,\n",
       "  751,\n",
       "  752,\n",
       "  753,\n",
       "  754,\n",
       "  755,\n",
       "  756,\n",
       "  757,\n",
       "  758,\n",
       "  759,\n",
       "  760,\n",
       "  761,\n",
       "  762,\n",
       "  763,\n",
       "  764,\n",
       "  765,\n",
       "  766,\n",
       "  767,\n",
       "  768,\n",
       "  769,\n",
       "  770,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  774,\n",
       "  775,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  780,\n",
       "  781,\n",
       "  782,\n",
       "  783,\n",
       "  784,\n",
       "  785,\n",
       "  786,\n",
       "  787,\n",
       "  788,\n",
       "  789,\n",
       "  790,\n",
       "  791,\n",
       "  792,\n",
       "  793,\n",
       "  794,\n",
       "  795,\n",
       "  796,\n",
       "  797,\n",
       "  798,\n",
       "  799,\n",
       "  800,\n",
       "  801,\n",
       "  802,\n",
       "  803,\n",
       "  804,\n",
       "  805,\n",
       "  806,\n",
       "  807,\n",
       "  808,\n",
       "  809,\n",
       "  810,\n",
       "  811,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  816,\n",
       "  817,\n",
       "  818,\n",
       "  819,\n",
       "  820,\n",
       "  821,\n",
       "  822,\n",
       "  823,\n",
       "  824,\n",
       "  825,\n",
       "  826,\n",
       "  827,\n",
       "  828,\n",
       "  829,\n",
       "  830,\n",
       "  831,\n",
       "  832,\n",
       "  833,\n",
       "  834,\n",
       "  835,\n",
       "  836,\n",
       "  837,\n",
       "  838,\n",
       "  839,\n",
       "  840,\n",
       "  841,\n",
       "  842,\n",
       "  843,\n",
       "  844,\n",
       "  845,\n",
       "  846,\n",
       "  847,\n",
       "  848,\n",
       "  849,\n",
       "  850,\n",
       "  851,\n",
       "  852,\n",
       "  853,\n",
       "  854,\n",
       "  855,\n",
       "  856,\n",
       "  857,\n",
       "  858,\n",
       "  859,\n",
       "  860,\n",
       "  861,\n",
       "  862,\n",
       "  863,\n",
       "  864,\n",
       "  865,\n",
       "  866,\n",
       "  867,\n",
       "  868,\n",
       "  869,\n",
       "  870,\n",
       "  871,\n",
       "  872,\n",
       "  873,\n",
       "  874,\n",
       "  875,\n",
       "  876,\n",
       "  877,\n",
       "  878,\n",
       "  879,\n",
       "  880,\n",
       "  881,\n",
       "  882,\n",
       "  883,\n",
       "  884,\n",
       "  885,\n",
       "  886,\n",
       "  887,\n",
       "  888,\n",
       "  889,\n",
       "  890,\n",
       "  891,\n",
       "  892,\n",
       "  893,\n",
       "  894,\n",
       "  895,\n",
       "  896,\n",
       "  897,\n",
       "  898,\n",
       "  899,\n",
       "  900,\n",
       "  901,\n",
       "  902,\n",
       "  903,\n",
       "  904,\n",
       "  905,\n",
       "  906,\n",
       "  907,\n",
       "  908,\n",
       "  909,\n",
       "  910,\n",
       "  911,\n",
       "  912,\n",
       "  913,\n",
       "  914,\n",
       "  915,\n",
       "  916,\n",
       "  917,\n",
       "  918,\n",
       "  919,\n",
       "  920,\n",
       "  921,\n",
       "  922,\n",
       "  923,\n",
       "  924,\n",
       "  925,\n",
       "  926,\n",
       "  927,\n",
       "  928,\n",
       "  929,\n",
       "  930,\n",
       "  931,\n",
       "  932,\n",
       "  933,\n",
       "  934,\n",
       "  935,\n",
       "  936,\n",
       "  937,\n",
       "  938,\n",
       "  939,\n",
       "  940,\n",
       "  941,\n",
       "  942,\n",
       "  943,\n",
       "  944,\n",
       "  945,\n",
       "  946,\n",
       "  947,\n",
       "  948,\n",
       "  949,\n",
       "  950,\n",
       "  951,\n",
       "  952,\n",
       "  953,\n",
       "  954,\n",
       "  955,\n",
       "  956,\n",
       "  957,\n",
       "  958,\n",
       "  959,\n",
       "  960,\n",
       "  961,\n",
       "  962,\n",
       "  963,\n",
       "  964,\n",
       "  965,\n",
       "  966,\n",
       "  967,\n",
       "  968,\n",
       "  969,\n",
       "  970,\n",
       "  971,\n",
       "  972,\n",
       "  973,\n",
       "  974,\n",
       "  975,\n",
       "  976,\n",
       "  977,\n",
       "  978,\n",
       "  979,\n",
       "  980,\n",
       "  981,\n",
       "  982,\n",
       "  983,\n",
       "  984,\n",
       "  985,\n",
       "  986,\n",
       "  987,\n",
       "  988,\n",
       "  989,\n",
       "  990,\n",
       "  991,\n",
       "  992,\n",
       "  993,\n",
       "  994,\n",
       "  995,\n",
       "  996,\n",
       "  997,\n",
       "  998,\n",
       "  999,\n",
       "  ...]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20b9ad63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5087422991787031],\n",
       " [[0.4745979047501622,\n",
       "   0.7885986845537435,\n",
       "   0.5788823331096964,\n",
       "   0.19289027430121009]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err, err2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b059fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['harness_truthfulqa_mc_0', 'harness_gsm8k_5', 'harness_arc_challenge_25', 'harness_hellaswag_10'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscenarios_position['lb'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c071b44-1410-4cb8-9d20-2f5a3ed9c5c9",
   "metadata": {},
   "source": [
    "Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "11008304-b6db-4b55-863d-c9968a0b76ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 226.82it/s]\n",
      "4it [00:00, 238.49it/s]\n",
      "4it [00:00, 238.76it/s]\n",
      "4it [00:00, 241.34it/s]\n",
      "4it [00:00, 237.12it/s]\n",
      "4it [00:00, 239.30it/s]\n",
      "4it [00:00, 250.10it/s]\n",
      "4it [00:00, 231.18it/s]\n",
      "4it [00:00, 232.24it/s]\n",
      "4it [00:00, 256.22it/s]\n",
      "4it [00:00, 244.47it/s]\n",
      "4it [00:00, 254.68it/s]\n",
      "4it [00:00, 241.20it/s]\n",
      "4it [00:00, 251.30it/s]\n",
      "4it [00:00, 257.48it/s]\n",
      "4it [00:00, 246.99it/s]\n",
      "4it [00:00, 241.92it/s]\n",
      "4it [00:00, 256.06it/s]\n",
      "4it [00:00, 251.80it/s]\n",
      "4it [00:00, 247.73it/s]\n",
      "4it [00:00, 253.67it/s]\n",
      "4it [00:00, 233.37it/s]\n",
      "4it [00:00, 239.56it/s]\n",
      "4it [00:00, 256.23it/s]\n",
      "4it [00:00, 250.82it/s]\n",
      "4it [00:00, 240.32it/s]\n",
      "4it [00:00, 250.91it/s]\n",
      "4it [00:00, 246.78it/s]\n",
      "4it [00:00, 256.20it/s]\n",
      "4it [00:00, 239.24it/s]\n",
      "4it [00:00, 244.87it/s]\n",
      "4it [00:00, 219.11it/s]\n",
      "4it [00:00, 244.90it/s]\n",
      "4it [00:00, 254.10it/s]\n",
      "4it [00:00, 259.78it/s]\n",
      "4it [00:00, 190.48it/s]\n",
      "4it [00:00, 220.79it/s]\n",
      "4it [00:00, 232.94it/s]\n",
      "4it [00:00, 239.78it/s]\n",
      "4it [00:00, 248.91it/s]\n",
      "4it [00:00, 234.47it/s]\n",
      "4it [00:00, 184.98it/s]\n",
      "4it [00:00, 227.52it/s]\n",
      "4it [00:00, 252.62it/s]\n",
      "4it [00:00, 253.32it/s]\n",
      "4it [00:00, 257.11it/s]\n",
      "4it [00:00, 244.37it/s]\n",
      "4it [00:00, 257.13it/s]\n",
      "4it [00:00, 235.57it/s]\n",
      "4it [00:00, 236.54it/s]\n",
      "4it [00:00, 243.57it/s]\n",
      "4it [00:00, 249.15it/s]\n",
      "4it [00:00, 243.32it/s]\n",
      "4it [00:00, 253.70it/s]\n",
      "4it [00:00, 232.12it/s]\n",
      "4it [00:00, 236.99it/s]\n",
      "4it [00:00, 241.13it/s]\n",
      "4it [00:00, 258.64it/s]\n",
      "4it [00:00, 253.66it/s]\n",
      "4it [00:00, 259.05it/s]\n",
      "4it [00:00, 258.85it/s]\n",
      "4it [00:00, 236.86it/s]\n",
      "4it [00:00, 244.23it/s]\n",
      "4it [00:00, 235.16it/s]\n",
      "4it [00:00, 249.18it/s]\n",
      "4it [00:00, 246.37it/s]\n",
      "4it [00:00, 252.77it/s]\n",
      "4it [00:00, 251.22it/s]\n",
      "4it [00:00, 255.51it/s]\n",
      "4it [00:00, 241.24it/s]\n"
     ]
    }
   ],
   "source": [
    "clustering = 'irt' # 'correct.' or 'irt'\n",
    "\n",
    "for s_displ in range(10):\n",
    "    for anchor_num in [10, 15, 20, 30, 40, 50, 100]:\n",
    "        all_anchor_points = {}\n",
    "        all_anchor_weights = {}\n",
    "        \n",
    "        for scenario in lb_scenarios['lb']:\n",
    "            scenario_pos = {scenario: subscenarios_position['lb'][scenario]}\n",
    "            anchor_points, anchor_weights = compute_anchor_points(clustering, [scenario], scenario_pos, balance_weights, Y_train, anchor_num, random_state+s_displ)\n",
    "            all_anchor_points[scenario] = anchor_points[scenario]\n",
    "            all_anchor_weights[scenario] = anchor_weights[scenario]\n",
    "\n",
    "        # save to tinybenchmark_lb file, putting 'seen_examples', 'examples_weights', 'irt_parameters', 'scenarios_position', 'subscenarios_position', 'optimal_lambdas'\n",
    "        tinybenchmark_lb = {'seen_examples':all_anchor_points,\n",
    "                            'examples_weights':all_anchor_weights,\n",
    "                            'scenarios_position':scenarios_position,\n",
    "                            'subscenarios_position':subscenarios_position,\n",
    "                            }\n",
    "\n",
    "        with open(f'data/tinybenchmark_rep/anchor_{anchor_num}_{random_state+s_displ}.pickle', 'wb') as handle:\n",
    "            pickle.dump(tinybenchmark_lb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        all_anchors = []\n",
    "        all_weights = []\n",
    "        for k in all_anchor_points.keys():\n",
    "            all_anchors.append(all_anchor_points[k])\n",
    "            all_weights.append(all_anchor_weights[k])\n",
    "\n",
    "        all_anchors = np.hstack(all_anchors)\n",
    "        all_weights = np.hstack(all_weights)\n",
    "\n",
    "        A, B, Theta = load_irt_parameters(f'data/irt_model_lb/')\n",
    "        ind_D = np.argmin(np.array(err))\n",
    "\n",
    "        scenarios_pos = {}\n",
    "        subs_position = {}\n",
    "        for s in lb_scenarios['lb']:\n",
    "            scenarios_pos[lb_scenario_to_readable[s]] = subscenarios_position['lb'][s]\n",
    "            subs_position[lb_scenario_to_readable[s]] = {s: subscenarios_position['lb'][s]}\n",
    "            all_anchor_weights[lb_scenario_to_readable[s]] = all_anchor_weights.pop(s)\n",
    "\n",
    "        optimal_lambdas = estimate_lambdas(err2[ind_D], Y_train, scenarios_pos, anchor_num)\n",
    "\n",
    "        with open(f'data/tinybenchmark_rep/tinybenchmark_{anchor_num}_{random_state+s_displ}.pickle', 'wb') as handle:\n",
    "            pickle.dump({'lb': {'seen_examples': all_anchors, \n",
    "                                'examples_weights': all_anchor_weights, \n",
    "                                'irt_parameters': {'A': A, 'B': B}, \n",
    "                                'scenarios_position': scenarios_pos, \n",
    "                                'subscenarios_position': subs_position, \n",
    "                                'optimal_lambdas': optimal_lambdas}\n",
    "                            }, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5eb2301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'harness_truthfulqa_mc_0': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  477,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  591,\n",
       "  592,\n",
       "  593,\n",
       "  594,\n",
       "  595,\n",
       "  596,\n",
       "  597,\n",
       "  598,\n",
       "  599,\n",
       "  600,\n",
       "  601,\n",
       "  602,\n",
       "  603,\n",
       "  604,\n",
       "  605,\n",
       "  606,\n",
       "  607,\n",
       "  608,\n",
       "  609,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  613,\n",
       "  614,\n",
       "  615,\n",
       "  616,\n",
       "  617,\n",
       "  618,\n",
       "  619,\n",
       "  620,\n",
       "  621,\n",
       "  622,\n",
       "  623,\n",
       "  624,\n",
       "  625,\n",
       "  626,\n",
       "  627,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  639,\n",
       "  640,\n",
       "  641,\n",
       "  642,\n",
       "  643,\n",
       "  644,\n",
       "  645,\n",
       "  646,\n",
       "  647,\n",
       "  648,\n",
       "  649,\n",
       "  650,\n",
       "  651,\n",
       "  652,\n",
       "  653,\n",
       "  654,\n",
       "  655,\n",
       "  656,\n",
       "  657,\n",
       "  658,\n",
       "  659,\n",
       "  660,\n",
       "  661,\n",
       "  662,\n",
       "  663,\n",
       "  664,\n",
       "  665,\n",
       "  666,\n",
       "  667,\n",
       "  668,\n",
       "  669,\n",
       "  670,\n",
       "  671,\n",
       "  672,\n",
       "  673,\n",
       "  674,\n",
       "  675,\n",
       "  676,\n",
       "  677,\n",
       "  678,\n",
       "  679,\n",
       "  680,\n",
       "  681,\n",
       "  682,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  689,\n",
       "  690,\n",
       "  691,\n",
       "  692,\n",
       "  693,\n",
       "  694,\n",
       "  695,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  699,\n",
       "  700,\n",
       "  701,\n",
       "  702,\n",
       "  703,\n",
       "  704,\n",
       "  705,\n",
       "  706,\n",
       "  707,\n",
       "  708,\n",
       "  709,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  713,\n",
       "  714,\n",
       "  715,\n",
       "  716,\n",
       "  717,\n",
       "  718,\n",
       "  719,\n",
       "  720,\n",
       "  721,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  726,\n",
       "  727,\n",
       "  728,\n",
       "  729,\n",
       "  730,\n",
       "  731,\n",
       "  732,\n",
       "  733,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  737,\n",
       "  738,\n",
       "  739,\n",
       "  740,\n",
       "  741,\n",
       "  742,\n",
       "  743,\n",
       "  744,\n",
       "  745,\n",
       "  746,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  750,\n",
       "  751,\n",
       "  752,\n",
       "  753,\n",
       "  754,\n",
       "  755,\n",
       "  756,\n",
       "  757,\n",
       "  758,\n",
       "  759,\n",
       "  760,\n",
       "  761,\n",
       "  762,\n",
       "  763,\n",
       "  764,\n",
       "  765,\n",
       "  766,\n",
       "  767,\n",
       "  768,\n",
       "  769,\n",
       "  770,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  774,\n",
       "  775,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  780,\n",
       "  781,\n",
       "  782,\n",
       "  783,\n",
       "  784,\n",
       "  785,\n",
       "  786,\n",
       "  787,\n",
       "  788,\n",
       "  789,\n",
       "  790,\n",
       "  791,\n",
       "  792,\n",
       "  793,\n",
       "  794,\n",
       "  795,\n",
       "  796,\n",
       "  797,\n",
       "  798,\n",
       "  799,\n",
       "  800,\n",
       "  801,\n",
       "  802,\n",
       "  803,\n",
       "  804,\n",
       "  805,\n",
       "  806,\n",
       "  807,\n",
       "  808,\n",
       "  809,\n",
       "  810,\n",
       "  811,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  816]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_position['truthfulqa']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651edaf8-af28-4e6f-92d6-192477c0aa44",
   "metadata": {},
   "source": [
    "Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5a468b8f-950b-41c5-b009-e9b3d913b0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'harness_truthfulqa_mc_0': array([541, 596, 632, 810, 231, 500, 667, 357, 660, 548, 746, 222, 159,\n",
       "         60,  52, 703, 459, 435, 735, 653, 171,  89,   7, 268, 198, 220,\n",
       "        190, 614, 255, 524, 139, 425, 346, 727, 635,   9, 261, 476, 310,\n",
       "        405, 223, 491, 694, 133, 805, 561, 671, 568,  31, 285,  71, 188,\n",
       "        351,  93, 196, 452, 615, 329,  39, 130, 443, 765,  59, 336, 565,\n",
       "        609,  85, 309, 257, 465, 610, 354, 716, 670, 802,  68, 337, 599,\n",
       "        471, 633, 725, 742, 717, 157, 560,  64, 755, 480, 680, 816, 684,\n",
       "        175, 415, 704, 164, 797, 693, 747, 103, 135]),\n",
       " 'harness_gsm8k_5': array([ 494, 1257, 1101,  696, 1187,  680, 1058, 1233,  406,  569,  915,\n",
       "        1304,    7, 1020,  504,  255,  949,   32,  299, 1074,  349,  560,\n",
       "        1141,  445,  377,  803, 1183,  139,  426,  314,  921,  154,  240,\n",
       "         448,  801,  727,  924,  753, 1184,  834, 1242, 1259,  513,  789,\n",
       "         131,  435,  674,  490,  611,  591,  817,  956,  106,  198,  321,\n",
       "         543,  936,  132,   91,  818,  248,  501, 1195,   30,  973, 1061,\n",
       "        1230, 1253,  217,  826,  947,  645,  157,   98, 1076, 1249,  646,\n",
       "         444,  291, 1270, 1310,  235, 1173,  104, 1055, 1084,  735, 1065,\n",
       "         354,  728, 1103,  110,  721, 1053,  932,  508,  926,  332,  101,\n",
       "        1165]),\n",
       " 'harness_arc_challenge_25': array([ 438,  769, 1134,  211,  166,  857,   90,  445,  309,  213,  229,\n",
       "         141,  906,  853,  735, 1047,  837,  377,  485,   84, 1052, 1056,\n",
       "         980, 1109,  759,  404,  473,  746,   40,  471,    6,  131,  245,\n",
       "         995,  561,  776,  764,  770,  300,  578,  856,  437, 1129, 1094,\n",
       "         108,  823,  939, 1045,  544,  847,  613,  606,  933,  951,  331,\n",
       "         865,  130,  554,  505,  152,  751,  750,   75,  234,  732,  950,\n",
       "         286, 1118,  375,  121,  387,   59,  334,  673,  431,  440,  123,\n",
       "          57,   45,   38,  349,  528,  353,  407,  633,   58,   44,  503,\n",
       "         185,  901,  523, 1055,  788,  156,  822,  516, 1016,  310,  151,\n",
       "         163]),\n",
       " 'harness_hellaswag_10': array([  813,  6765,  9403,  6892,  5820,    76,  9613,   140,  2614,\n",
       "         6629,  9178,  5473,  9905, 10007,  2804,  7269,  5225,   340,\n",
       "         9726,  3736,  8733,  9840,  2269,  4558,  3138,  8965,  3722,\n",
       "         2094,  8379,  6521,  3544,  7036,  5320,  2687,  1661,  9647,\n",
       "         6572,  7971,  7865,  9659,  7142,  7327,  3872,  5470,  3242,\n",
       "         1488,  8487,  5737,  2836,  1328,   905,  8503,  4970,  5236,\n",
       "         5541,  2509,   429,  8878,  4342,   244,  5360,  3692,   349,\n",
       "         6270,  1786,  5441,  4008,   742,  2777,  9544,  1466,  8894,\n",
       "          335,  5481,  1861,  9904,  4252,  1815,  4990,  5995,  8472,\n",
       "         7847,   737,  2875,  1384,   793,  8654,  5923,  8501,  5553,\n",
       "         7270,   275,  5192,  5599,  7002,  6961,  8621,  4604,  2344,\n",
       "         3447])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinybenchmark_lb['seen_examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1efbfa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['harness_truthfulqa_mc_0', 'harness_gsm8k_5', 'harness_arc_challenge_25', 'harness_hellaswag_10'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinybenchmark_lb['seen_examples'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97b66dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4121, 10073, 12711, 10200,  9128,  3384, 12921,  3448,  5922,\n",
       "        9937, 12486,  8781, 13213, 13315,  6112, 10577,  8533,  3648,\n",
       "       13034,  7044, 12041, 13148,  5577,  7866,  6446, 12273,  7030,\n",
       "        5402, 11687,  9829,  6852, 10344,  8628,  5995,  4969, 12955,\n",
       "        9880, 11279, 11173, 12967, 10450, 10635,  7180,  8778,  6550,\n",
       "        4796, 11795,  9045,  6144,  4636,  4213, 11811,  8278,  8544,\n",
       "        8849,  5817,  3737, 12186,  7650,  3552,  8668,  7000,  3657,\n",
       "        9578,  5094,  8749,  7316,  4050,  6085, 12852,  4774, 12202,\n",
       "        3643,  8789,  5169, 13212,  7560,  5123,  8298,  9303, 11780,\n",
       "       11155,  4045,  6183,  4692,  4101, 11962,  9231, 11809,  8861,\n",
       "       10578,  3583,  8500,  8907, 10310, 10269, 11929,  7912,  5652,\n",
       "        6755])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinybenchmark_lb['seen_examples'][scenario] + tinybenchmark_lb['subscenarios_position']['lb'][scenario][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac7f2a-9288-4ef5-aa57-7280523cfd4c",
   "metadata": {},
   "source": [
    "Using anchor points to estimate performance in the test set and reporting the average prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2fcaa571",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'harness_truthfulqa_mc_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scenario \u001b[38;5;129;01min\u001b[39;00m tinybenchmark_lb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseen_examples\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      2\u001b[0m     Y_anchor \u001b[38;5;241m=\u001b[39m Y_test[:,subscenarios_position[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlb\u001b[39m\u001b[38;5;124m'\u001b[39m][scenario]][:,all_anchor_points[scenario]]\n\u001b[0;32m----> 3\u001b[0m     Y_hat \u001b[38;5;241m=\u001b[39m (Y_anchor\u001b[38;5;241m*\u001b[39m\u001b[43mall_anchor_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m     Y_true \u001b[38;5;241m=\u001b[39m (Y_test)[:,subscenarios_position[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlb\u001b[39m\u001b[38;5;124m'\u001b[39m][scenario]]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscenario: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscenario\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, avg. error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mabs(Y_hat\u001b[38;5;241m-\u001b[39mY_true)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'harness_truthfulqa_mc_0'"
     ]
    }
   ],
   "source": [
    "for scenario in tinybenchmark_lb['seen_examples'].keys():\n",
    "    Y_anchor = Y_test[:,subscenarios_position['lb'][scenario]][:,all_anchor_points[scenario]]\n",
    "    Y_hat = (Y_anchor*all_anchor_weights[scenario]).sum(axis=1)\n",
    "    Y_true = (Y_test)[:,subscenarios_position['lb'][scenario]].mean(axis=1)\n",
    "\n",
    "    print(f\"scenario: {scenario}, avg. error: {np.abs(Y_hat-Y_true).mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
